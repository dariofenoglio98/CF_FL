{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1672891\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "\n",
    "config = utils.config_tests[\"mnist\"][\"net\"]\n",
    "model = utils.models[\"net\"]\n",
    "model = model(config=config).to(\"mps\")\n",
    "\n",
    "# calculate nummber of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 726211\n"
     ]
    }
   ],
   "source": [
    "config = utils.config_tests[\"mnist\"][\"predictor\"]\n",
    "model = utils.models[\"predictor\"]\n",
    "model = model(config=config).to(\"mps\")\n",
    "\n",
    "# calculate nummber of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 291522\n"
     ]
    }
   ],
   "source": [
    "# calculate nummber of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/train_features_FL.npy')\n",
    "y = np.load('data/train_tasks_FL.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values CIFAR10: -11.886337280273438\n",
      "Max values CIFAR10: 24.897098541259766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## reading cifar10\n",
    "# CIFAR10\n",
    "x = np.load('data/train_features_cifar10.npy')\n",
    "y = np.load('data/train_tasks_cifar10.npy')\n",
    "# pick random 10000 indexes\n",
    "idx = np.random.choice(x.shape[0], 10000, replace=False)\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "# create a unique dataset, with y=Labels and x=Features from 1 to 1000\n",
    "df_CIFAR10 = pd.DataFrame(x, columns=[str(i) for i in range(1000)])\n",
    "df_CIFAR10['Labels'] = y\n",
    "df_CIFAR10 = df_CIFAR10.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "XXXXX = df_CIFAR10.drop('Labels', axis=1)\n",
    "min_values_cifar10 = XXXXX.min().values\n",
    "max_values_cifar10 = XXXXX.max().values\n",
    "print(f\"Min values CIFAR10: {min(min_values_cifar10)}\")\n",
    "print(f\"Max values CIFAR10: {max(max_values_cifar10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.423042 , 12.600371 ,  8.072652 ,  9.027553 , 12.251503 ,\n",
       "        9.412625 ,  4.374051 ,  8.022254 ,  8.820903 , 14.572547 ,\n",
       "       18.415415 , 11.8148775, 10.213047 , 12.746513 , 10.198513 ,\n",
       "        9.072144 , 14.926754 ,  9.434887 , 10.594098 , 10.808314 ,\n",
       "       11.800672 , 15.662813 , 12.325638 , 10.182576 , 10.051552 ,\n",
       "       10.429396 , 12.341191 , 10.919868 , 11.383953 ,  9.62078  ,\n",
       "       12.641973 , 11.7588215, 18.605917 ,  8.232991 ,  8.178155 ,\n",
       "       10.142697 ,  9.711519 ,  8.018498 , 11.993644 ,  7.7086954,\n",
       "       10.10922  , 10.69895  , 10.827294 , 12.934413 ,  8.068943 ,\n",
       "        6.800266 ,  8.450273 , 12.084829 ,  5.7050962,  8.13478  ,\n",
       "        4.6484833, 12.340337 ,  8.344975 ,  7.455947 ,  7.0156026,\n",
       "        9.782901 ,  6.773877 ,  7.98915  ,  5.2508674, 14.677648 ,\n",
       "        9.556901 ,  9.972726 , 14.440763 , 10.136069 , 14.389955 ,\n",
       "        8.17452  , 10.531808 ,  6.106569 , 11.706444 ,  6.6199045,\n",
       "        7.1996427, 13.72882  ,  7.495158 , 15.475789 ,  7.8043923,\n",
       "        8.015189 ,  7.221599 ,  6.8278465, 17.109064 ,  9.117718 ,\n",
       "       16.91463  ,  8.873056 , 10.7706175, 11.824097 ,  9.356311 ,\n",
       "        9.130881 , 12.17669  ,  7.7712345,  7.8754706,  7.653214 ,\n",
       "        8.324069 , 11.9012375,  9.44431  ,  8.6236515, 11.779489 ,\n",
       "       13.942161 ,  9.355167 ,  7.654384 , 18.142841 , 14.652006 ,\n",
       "        9.403797 , 16.312561 ,  7.16964  , 13.946706 , 16.3572   ,\n",
       "        8.765242 ,  9.172639 ,  5.640151 ,  4.5128074,  5.9117665,\n",
       "        7.462715 , 12.33427  ,  4.899757 , 10.573688 , 10.485943 ,\n",
       "        8.130171 , 10.659076 , 10.2069025,  6.4493723,  7.8483105,\n",
       "       10.048124 ,  5.849475 ,  7.425576 ,  5.423279 ,  7.5695977,\n",
       "        5.8062406, 13.502331 , 12.597286 , 16.680561 , 11.441736 ,\n",
       "       10.276374 ,  9.071452 ,  8.40834  , 10.349765 , 13.548771 ,\n",
       "       15.433218 , 12.238259 ,  9.4819975, 10.357548 ,  9.299655 ,\n",
       "        9.8919115,  7.008736 , 12.56674  , 11.215012 , 10.182248 ,\n",
       "       12.035544 , 12.359167 , 15.739383 , 12.830032 , 13.033827 ,\n",
       "       11.281161 , 14.786806 , 21.604353 , 15.237174 , 19.796785 ,\n",
       "       13.230903 , 20.774921 , 16.610964 , 19.320301 , 13.446769 ,\n",
       "       12.6823225, 13.625623 , 15.808905 , 15.021238 , 17.727396 ,\n",
       "       17.967262 , 20.220217 , 21.076975 , 15.720018 , 12.659912 ,\n",
       "       12.211095 , 10.520379 , 11.871631 , 14.5541935, 14.035283 ,\n",
       "       15.432955 , 11.744743 , 14.05703  ,  8.76277  ,  8.781368 ,\n",
       "       10.523994 , 16.437313 , 12.568853 , 13.201035 , 11.769848 ,\n",
       "       12.037534 , 11.6381   , 13.867146 , 15.478898 , 12.151068 ,\n",
       "       13.665386 , 11.908258 ,  9.676747 , 12.204349 , 16.60849  ,\n",
       "        8.935873 ,  9.476754 , 11.076591 , 11.113335 ,  8.706856 ,\n",
       "       13.4322195,  8.850754 ,  9.743563 ,  9.565993 , 11.05031  ,\n",
       "       13.2370825, 15.878464 , 15.9591675, 13.19106  , 10.846144 ,\n",
       "       14.019334 , 11.70356  , 10.933071 , 14.377319 , 13.372836 ,\n",
       "       15.462259 , 16.464153 ,  9.529345 , 13.319782 , 11.696896 ,\n",
       "       14.5144205, 16.022778 , 11.727517 , 11.824114 , 11.997706 ,\n",
       "       13.278777 , 12.9261465, 13.593019 , 13.262985 , 13.09814  ,\n",
       "       12.147039 , 12.41028  , 11.601058 , 12.137214 , 17.644577 ,\n",
       "       15.961523 , 13.747489 , 13.004815 , 13.812068 , 11.120778 ,\n",
       "       14.925933 , 19.211996 , 13.58022  , 16.952475 , 13.260672 ,\n",
       "       10.732033 , 11.967375 , 17.942839 , 13.150628 , 11.464619 ,\n",
       "       11.021059 ,  8.88916  , 12.465601 , 16.39167  , 12.512536 ,\n",
       "       14.482495 , 11.428008 , 14.826544 ,  9.431978 , 13.301453 ,\n",
       "       14.794727 , 14.699086 , 16.985245 , 15.13378  , 17.01096  ,\n",
       "       10.933904 , 11.3807125, 10.78646  , 16.701368 , 13.155269 ,\n",
       "       10.696098 , 16.344585 , 14.129975 , 14.740769 , 16.830936 ,\n",
       "       15.363301 , 11.795459 , 15.764345 , 16.837313 , 12.681307 ,\n",
       "       14.501582 , 13.145063 , 12.919049 , 15.524696 , 15.161263 ,\n",
       "       12.80709  ,  9.810268 , 13.372867 , 12.891041 , 12.09371  ,\n",
       "       11.801185 , 15.40629  , 10.9673   , 12.093878 , 13.23808  ,\n",
       "       13.275379 , 11.030425 , 13.7553215, 11.737257 , 14.039867 ,\n",
       "        7.3089795,  7.5213885,  9.646495 ,  8.599441 ,  8.71519  ,\n",
       "        5.514182 ,  6.607811 , 12.834466 ,  7.291618 ,  7.5909886,\n",
       "       16.267426 ,  9.006324 ,  9.439698 ,  7.15926  ,  8.051319 ,\n",
       "        8.088756 ,  7.8305063,  9.640934 ,  9.272353 ,  8.354439 ,\n",
       "        8.33176  ,  4.9442296,  9.511994 ,  6.539385 ,  7.5671463,\n",
       "        9.677769 , 10.077647 , 11.036162 ,  3.6950805,  5.3213396,\n",
       "       15.661696 , 16.112076 ,  9.220209 , 11.438494 ,  6.0530567,\n",
       "       16.041002 , 11.547883 , 12.045012 , 10.05529  , 22.310757 ,\n",
       "        7.80537  , 14.312904 , 13.352009 ,  9.891354 ,  9.784887 ,\n",
       "       13.592736 , 10.995539 , 12.508782 ,  7.4699473, 12.881756 ,\n",
       "       12.044169 , 19.539776 , 15.257382 , 16.65123  , 15.670284 ,\n",
       "       11.521897 , 11.637071 , 12.905571 , 14.395081 , 13.844016 ,\n",
       "       12.385043 , 10.622556 , 11.257451 ,  8.666015 , 13.137334 ,\n",
       "       11.730389 , 12.335259 , 11.354957 , 14.101747 , 12.516272 ,\n",
       "       15.350858 , 15.516988 , 12.535978 , 13.976821 , 17.061357 ,\n",
       "       14.515347 , 17.432915 , 14.35492  , 12.218487 , 12.708157 ,\n",
       "       14.509602 , 13.763436 , 13.199323 , 13.313967 , 14.678148 ,\n",
       "       15.936587 , 17.12607  , 13.153656 , 10.037058 ,  7.423865 ,\n",
       "       12.921805 ,  6.936595 , 13.705724 ,  4.7894106,  7.1884527,\n",
       "        8.737328 ,  3.1286328,  5.9151635,  7.684082 , 11.460758 ,\n",
       "        8.826709 ,  4.8171597,  4.2927785, 10.744955 , 20.117786 ,\n",
       "       13.171628 ,  5.704103 , 14.9843645, 15.269016 ,  9.65403  ,\n",
       "        6.236513 ,  3.6341562,  5.9820952, 16.538197 ,  4.5123873,\n",
       "        4.0167494,  9.448283 ,  9.173247 , 10.294195 ,  9.639916 ,\n",
       "        4.8310704,  7.373378 ,  9.429036 ,  9.869491 ,  5.0878434,\n",
       "        5.8131547, 10.941274 , 11.219604 ,  6.82939  ,  6.9397283,\n",
       "        5.1307497,  8.489858 ,  8.393783 ,  9.122754 ,  7.2400713,\n",
       "        5.8758717, 10.579843 ,  9.685943 ,  9.140491 , 12.800261 ,\n",
       "        6.1707335,  5.543158 ,  6.37051  ,  6.5046587,  6.1337056,\n",
       "        4.976275 , 11.008603 ,  8.089436 ,  5.2120852, 10.238958 ,\n",
       "       10.558798 , 12.026506 ,  8.103722 ,  6.7788525,  3.9033296,\n",
       "        6.009597 ,  9.451349 ,  6.8879104,  4.279061 ,  7.789428 ,\n",
       "        6.864155 , 10.811963 ,  7.018636 ,  7.573482 ,  7.0212703,\n",
       "       12.213905 ,  9.236045 ,  3.6813154, 14.477719 ,  7.7647347,\n",
       "        7.657902 ,  7.3790917, 11.8840275, 13.985491 ,  4.4762745,\n",
       "        5.94586  ,  7.319223 ,  9.48066  ,  5.9847965, 13.017913 ,\n",
       "        9.096772 , 11.605709 , 16.19153  ,  7.0180254, 12.230394 ,\n",
       "        7.4350557,  9.063025 ,  6.8848233, 10.540604 ,  5.7859077,\n",
       "        9.289821 , 15.86549  ,  5.48397  , 12.855432 ,  9.991918 ,\n",
       "        6.7986927,  8.05094  ,  5.1537786,  7.016343 , 11.610358 ,\n",
       "        3.837495 , 10.1794405, 13.239135 , 11.934212 ,  5.425908 ,\n",
       "        6.855459 ,  4.816075 , 11.342717 ,  5.994981 ,  6.1969795,\n",
       "       17.32707  , 11.835393 , 10.737593 ,  6.8414717, 10.567204 ,\n",
       "        5.4684205,  9.997515 , 12.143566 , 10.479893 , 10.181096 ,\n",
       "        5.3357043, 11.031071 , 11.25723  ,  5.678773 , 10.142859 ,\n",
       "        6.548082 ,  4.6011395,  9.820372 ,  7.530714 ,  5.6161156,\n",
       "        9.52498  , 11.822717 ,  6.6116076,  2.6074805, 13.429269 ,\n",
       "        6.9678173, 11.687521 , 12.236306 ,  6.217203 ,  5.3865795,\n",
       "       14.934564 ,  8.090316 ,  7.2572107,  9.753131 ,  8.822268 ,\n",
       "        4.3493032,  5.3919234, 11.797848 ,  9.710548 ,  7.5174913,\n",
       "       14.289838 , 11.277067 ,  4.2258983,  7.097305 , 18.129316 ,\n",
       "       12.04495  , 11.859305 ,  7.3025494,  9.907678 , 14.360762 ,\n",
       "        8.1713505, 13.694214 , 11.869516 ,  8.750341 ,  6.1776495,\n",
       "       10.376714 ,  5.9566627, 12.777323 ,  6.7009377, 17.115627 ,\n",
       "        7.685646 ,  5.6558604,  4.678447 , 13.051437 , 11.012899 ,\n",
       "        9.845566 , 11.814046 , 10.831148 ,  3.5634387,  4.9834375,\n",
       "        2.3554754,  6.3050485,  5.0073667, 11.701597 , 10.426035 ,\n",
       "        9.914794 , 10.514083 , 10.794141 ,  8.4080715, 10.70974  ,\n",
       "       12.124867 ,  6.0017543,  7.649061 , 10.446056 ,  6.2472043,\n",
       "       12.787626 , 12.130221 ,  9.320525 ,  8.733866 ,  7.497744 ,\n",
       "        9.2445   ,  6.2801156,  9.64971  , 10.880744 , 12.291624 ,\n",
       "        4.8951187, 12.081722 ,  7.591355 ,  6.8205657,  7.8167315,\n",
       "        8.725654 ,  5.4441223,  8.378785 , 10.145955 ,  5.0335126,\n",
       "       10.775025 ,  8.389328 ,  6.877698 ,  5.619941 ,  7.693526 ,\n",
       "        8.503053 , 11.223856 ,  5.160133 , 13.762598 ,  5.5123944,\n",
       "       14.123771 ,  7.626049 , 11.954023 , 10.1791725, 10.346565 ,\n",
       "        9.0125885, 12.475377 ,  8.252673 , 11.535772 ,  8.916764 ,\n",
       "       11.139419 ,  7.4518204,  7.6208224,  4.718259 ,  7.6182995,\n",
       "        3.6023192, 12.7465   ,  6.379441 ,  6.222041 ,  9.460899 ,\n",
       "        6.3708296,  7.806422 ,  8.221031 ,  6.9791317,  6.83055  ,\n",
       "        5.6000247, 10.012414 ,  5.811095 , 14.767918 , 13.473534 ,\n",
       "        3.4461896, 13.873758 , 10.229213 ,  5.8937693,  6.25759  ,\n",
       "       15.011016 ,  7.3293357, 11.705369 , 10.009353 ,  9.642102 ,\n",
       "        9.64629  ,  9.155633 ,  8.768288 ,  5.9290986,  3.7535996,\n",
       "        9.92896  ,  4.9851313,  9.1948805, 11.0766945, 10.2240095,\n",
       "       24.897099 , 15.193479 , 12.723156 , 10.538682 ,  4.910994 ,\n",
       "       13.023977 ,  8.261494 ,  6.2064695, 10.114009 , 15.107657 ,\n",
       "        6.517258 , 12.397077 ,  5.539683 ,  8.409932 ,  8.757117 ,\n",
       "       11.051216 ,  7.6570263,  8.730093 , 12.77859  , 10.129924 ,\n",
       "        6.4150352,  9.067395 ,  5.3645887,  5.685756 , 12.538909 ,\n",
       "        6.155089 ,  9.2667055, 11.949318 ,  7.770288 ,  5.5924144,\n",
       "       14.889629 ,  4.4886036,  5.8040686, 10.472383 ,  3.5117674,\n",
       "        6.6665797,  6.1026163, 10.859845 ,  9.863116 , 12.052918 ,\n",
       "       12.588737 ,  3.9929051,  9.277023 ,  8.922599 ,  9.900033 ,\n",
       "        8.093022 ,  1.6383991,  8.64933  ,  9.55212  , 14.292775 ,\n",
       "        8.667101 , 12.7021   , 11.614873 ,  7.441296 , 11.659034 ,\n",
       "        8.729921 , 12.653685 ,  7.6056495,  7.7882857, 12.166946 ,\n",
       "        3.852963 ,  9.478823 ,  5.07191  ,  5.383152 ,  8.380659 ,\n",
       "       13.693639 ,  7.6161537,  8.496728 ,  9.195345 , 12.159496 ,\n",
       "        8.755552 ,  9.727746 , 10.75799  ,  4.2282763,  7.570674 ,\n",
       "        5.9286747, 12.676092 ,  4.6559176,  8.487344 ,  7.775654 ,\n",
       "        6.593262 ,  8.387504 , 14.227684 , 11.427807 ,  7.440069 ,\n",
       "        7.3672805,  8.01333  ,  5.284218 , 14.332948 , 11.307185 ,\n",
       "       10.738173 , 13.606696 ,  6.406048 ,  7.212828 ,  7.0790606,\n",
       "        7.642906 , 10.822718 , 10.9420595,  8.671939 ,  7.2859893,\n",
       "        6.1987143,  5.6765847,  9.009179 ,  7.4157877, 16.11624  ,\n",
       "       12.320805 , 10.25289  , 13.595716 ,  9.826943 ,  9.49929  ,\n",
       "        6.3116074,  7.439833 ,  7.958809 ,  4.6894765, 13.372389 ,\n",
       "        7.4342966,  5.3759665,  7.989113 ,  5.815736 ,  5.605028 ,\n",
       "       11.477219 ,  9.719017 ,  7.8892756, 10.329829 ,  5.607476 ,\n",
       "       11.332267 ,  6.5743613,  8.326176 , 12.8778715, 10.327348 ,\n",
       "       10.420819 ,  5.1718144, 10.474504 ,  6.0568705,  5.880212 ,\n",
       "        9.07606  , 11.113923 , 14.92093  , 12.204115 , 17.66122  ,\n",
       "        5.580801 ,  3.72529  , 15.600103 ,  9.457081 ,  4.001385 ,\n",
       "        7.0291877, 11.616187 ,  6.676191 ,  8.051293 ,  5.6591883,\n",
       "        5.4525123, 10.149748 ,  8.732568 ,  6.0015426,  8.96907  ,\n",
       "       10.757334 , 10.351458 ,  6.6998715, 11.964538 ,  5.587641 ,\n",
       "        6.848288 ,  8.5801325,  4.7317514, 12.487228 ,  6.4065604,\n",
       "        5.5349445, 11.401005 ,  7.6889076,  5.171593 , 10.182955 ,\n",
       "        6.539715 , 10.904772 ,  8.266155 ,  9.276647 ,  4.321268 ,\n",
       "        5.6931143,  9.257353 ,  7.9552407,  8.507618 , 11.541511 ,\n",
       "        8.972418 , 17.165365 , 10.463649 ,  5.887455 ,  9.892762 ,\n",
       "       10.975602 ,  7.803511 ,  8.26034  ,  4.79533  , 13.701771 ,\n",
       "        9.104507 , 12.267033 , 18.240162 ,  4.2651453,  8.196499 ,\n",
       "        6.356257 , 12.824478 , 10.636053 ,  5.241818 ,  9.419101 ,\n",
       "        6.6300707,  4.420878 ,  8.242999 ,  8.728341 ,  4.584805 ,\n",
       "        5.547815 ,  9.056444 ,  8.650375 ,  9.859706 ,  4.570766 ,\n",
       "        9.265617 ,  5.828682 ,  8.259866 ,  5.305815 ,  4.6342216,\n",
       "        7.6256595, 16.492048 , 10.138784 ,  5.729284 ,  7.116413 ,\n",
       "       14.425498 ,  7.807582 ,  6.057207 ,  6.812331 ,  8.20479  ,\n",
       "        8.094281 , 12.260614 , 14.128645 ,  7.046825 ,  7.763736 ,\n",
       "        8.41496  , 16.274887 ,  7.375737 , 13.130105 ,  7.749431 ,\n",
       "        7.509299 ,  5.433946 , 12.743695 ,  7.3867564, 18.11572  ,\n",
       "        6.113885 ,  9.384145 ,  7.965029 ,  4.263643 ,  7.0027604,\n",
       "        6.621828 ,  9.589724 ,  2.39603  ,  8.302287 ,  5.669666 ,\n",
       "        9.744057 , 10.360546 ,  6.3007126,  2.2440026,  6.0638347,\n",
       "        7.4143667,  8.33416  ,  6.726969 ,  3.7957487,  5.173444 ,\n",
       "        4.436493 , 10.07748  ,  8.519586 ,  6.4636483,  7.629194 ,\n",
       "       10.625359 ,  6.887586 ,  7.721892 ,  7.16249  ,  9.2082815,\n",
       "       11.207515 , 11.0120125,  8.268152 ,  7.153669 ,  9.965548 ,\n",
       "        4.5221815, 11.129084 , 10.360726 ,  7.259001 ,  6.78921  ,\n",
       "        8.141402 , 12.017651 ,  6.1856503,  5.6301665,  6.7742095,\n",
       "        7.447849 ,  3.1765065,  7.07635  ,  4.7499003,  7.623963 ,\n",
       "        4.056703 ,  6.805322 ,  7.9245534,  5.9127517,  8.997938 ,\n",
       "        8.449005 ,  5.836143 ,  5.0545344,  5.99448  , 14.902458 ,\n",
       "        9.28848  ,  9.4769125,  7.6576066,  8.22522  ,  5.812787 ,\n",
       "        7.480289 ,  8.04259  ,  3.5099022,  5.089172 ,  8.491042 ,\n",
       "        5.7677307, 11.871046 ,  6.581378 ,  7.702912 ,  9.055534 ,\n",
       "        8.379984 ,  5.165582 ,  6.8364434, 11.858059 ,  8.353548 ,\n",
       "        8.455578 ,  8.471602 ,  9.079713 ,  7.376801 ,  5.4419966],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XXXXX.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/782 [00:04<26:43,  2.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m             task_labels\u001b[38;5;241m.\u001b[39mappend(tasks\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(features), np\u001b[38;5;241m.\u001b[39mconcatenate(task_labels)\n\u001b[0;32m---> 89\u001b[0m train_features, train_tasks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Step 4: Save the embeddings and labels\u001b[39;00m\n\u001b[1;32m     92\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_features.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, train_features)\n",
      "Cell \u001b[0;32mIn[19], line 83\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, tasks \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     82\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 83\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     85\u001b[0m     task_labels\u001b[38;5;241m.\u001b[39mappend(tasks\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torchvision/models/resnet.py:100\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
    "])\n",
    "\n",
    "# Creating the custom dataset\n",
    "class CustomDSpritesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.dataset = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target_label = self.dataset[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            image = preprocess(image)\n",
    "\n",
    "        return image, torch.tensor(target_label, dtype=torch.float32)\n",
    "    \n",
    "# # Creating the custom dataset\n",
    "# class CustomDSpritesDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.dataset = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = self.dataset[idx][0]\n",
    "#         target_label = self.dataset[idx][1]\n",
    "\n",
    "#         return image, torch.tensor(target_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Prepare the MNIST dataset\n",
    "# [Include the CustomMNISTDataset class from the previous code snippet here]\n",
    "\n",
    "# Load dSprites dataset\n",
    "# load np arrays\n",
    "data = datasets.CIFAR10(root=os.path.expanduser(\"~/.cache\"), download=True, train=True, transform=None)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(custom_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Step 2: Prepare ResNet18 model for feature extraction\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Create custom datasets\n",
    "custom_train_dataset = CustomDSpritesDataset(data, transform=preprocess)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 3: Extract features\n",
    "def extract_features(data_loader):\n",
    "    features = []\n",
    "    task_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, tasks in tqdm(data_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            features.append(out.cpu().numpy())\n",
    "            task_labels.append(tasks.numpy())\n",
    "\n",
    "    return np.concatenate(features), np.concatenate(task_labels)\n",
    "\n",
    "train_features, train_tasks = extract_features(train_loader)\n",
    "\n",
    "# Step 4: Save the embeddings and labels\n",
    "np.save('train_features.npy', train_features)\n",
    "np.save('train_tasks.npy', train_tasks)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:06<00:00, 11.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "data = datasets.CIFAR10(root=os.path.expanduser(\"~/.cache\"), download=True, train=True, transform=None)\n",
    "\n",
    "# Step 2: Prepare ResNet18 model for feature extraction\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Creating the custom dataset\n",
    "class CustomDSpritesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.dataset = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target_label = self.dataset[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = preprocess(image)\n",
    "\n",
    "        return image, torch.tensor(target_label, dtype=torch.float32)\n",
    "\n",
    "# Create custom datasets\n",
    "custom_train_dataset = CustomDSpritesDataset(data, transform=preprocess)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(custom_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 3: Extract features\n",
    "def extract_features(data_loader):\n",
    "    features = []\n",
    "    task_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, tasks in tqdm(data_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            features.append(out.cpu().numpy())\n",
    "            task_labels.append(tasks.numpy())\n",
    "\n",
    "    return np.concatenate(features), np.concatenate(task_labels)\n",
    "\n",
    "train_features, train_tasks = extract_features(train_loader)\n",
    "\n",
    "# Step 4: Save the embeddings and labels\n",
    "np.save('data/train_features_cifar10.npy', train_features)\n",
    "np.save('data/train_tasks_cifar10.npy', train_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 11689512\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of resnet18\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "    import utils\n",
    "    import torch\n",
    "   \n",
    "    # model and history folder\n",
    "    model = utils.models[\"net\"]\n",
    "    train_fn = utils.trainings[\"net\"]\n",
    "    evaluate_fn = utils.evaluations[\"net\"]\n",
    "    plot_fn = utils.plot_functions[\"net\"]\n",
    "    config = utils.config_tests[\"cifar10\"][\"net\"]\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # check if metrics.csv exists otherwise delete it\n",
    "    utils.check_and_delete_metrics_file(config['history_folder'] + f\"client_{\"2cluster\"}_{\"1\"}\", question=False)\n",
    "\n",
    "    # check gpu and set manual seed\n",
    "    device = utils.check_gpu(manual_seed=True)\n",
    "\n",
    "    # load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, num_examples = utils.load_data(\n",
    "        client_id=str(\"1\"), device=device, type=\"2cluster\", dataset=\"cifar10\")\n",
    "\n",
    "    # Model\n",
    "    model = model(config=config).to(device)\n",
    "\n",
    "    # Optimizer and Loss function\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   10 / 100, Cost : 6.2459, Acc : 92.02 %, Validity : 9.13 %, Val Cost : 0.2383, Val Acc : 98.48 % , Val Validity : 3.03 %\n",
      "Epoch   20 / 100, Cost : 5.8245, Acc : 97.72 %, Validity : 9.13 %, Val Cost : 0.1174, Val Acc : 95.45 % , Val Validity : 3.03 %\n",
      "Epoch   30 / 100, Cost : 5.6180, Acc : 98.48 %, Validity : 10.27 %, Val Cost : 0.0862, Val Acc : 98.48 % , Val Validity : 6.06 %\n",
      "Epoch   40 / 100, Cost : 5.4067, Acc : 98.48 %, Validity : 13.31 %, Val Cost : 0.0728, Val Acc : 98.48 % , Val Validity : 3.03 %\n",
      "Epoch   50 / 100, Cost : 5.2624, Acc : 99.24 %, Validity : 9.89 %, Val Cost : 0.0652, Val Acc : 98.48 % , Val Validity : 4.55 %\n",
      "Epoch   60 / 100, Cost : 5.0947, Acc : 99.24 %, Validity : 7.98 %, Val Cost : 0.0575, Val Acc : 98.48 % , Val Validity : 10.61 %\n",
      "Epoch   70 / 100, Cost : 4.9487, Acc : 99.24 %, Validity : 11.03 %, Val Cost : 0.0575, Val Acc : 96.97 % , Val Validity : 9.09 %\n",
      "Epoch   80 / 100, Cost : 4.8610, Acc : 99.62 %, Validity : 11.41 %, Val Cost : 0.0583, Val Acc : 96.97 % , Val Validity : 6.06 %\n",
      "Epoch   90 / 100, Cost : 4.7428, Acc : 99.24 %, Validity : 11.79 %, Val Cost : 0.0562, Val Acc : 96.97 % , Val Validity : 12.12 %\n",
      "Epoch  100 / 100, Cost : 4.7441, Acc : 99.62 %, Validity : 11.79 %, Val Cost : 0.0585, Val Acc : 96.97 % , Val Validity : 3.03 %\n"
     ]
    }
   ],
   "source": [
    "model_trained, train_loss, val_loss, acc, acc_prime, acc_val, _ = train_fn(\n",
    "                model, loss_fn, optimizer, X_train, y_train, \n",
    "                X_val, y_val, n_epochs=100, print_info=False, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.nn.functional.one_hot(torch.tensor(np.random.randint(0, 9, size=20)), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CF_FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
