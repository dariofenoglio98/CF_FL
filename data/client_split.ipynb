{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subdivision of the dataset into N institutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes dataset: (70692, 22)\n",
      "Breast cancer dataset: (569, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Diabeters\n",
    "df_train = pd.read_csv('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')\n",
    "df_train = df_train.rename(columns={'Diabetes_binary': 'Labels'})\n",
    "# Breast cancer\n",
    "X_breast = pd.read_csv('X_breast.csv')\n",
    "y_breast = pd.read_csv('y_breast.csv')\n",
    "y_breast['Diagnosis'] = y_breast['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# add labels to X_breast with the same name as in df_train\n",
    "df_train_breast = pd.DataFrame(X_breast)\n",
    "df_train_breast['Labels'] = y_breast['Diagnosis']\n",
    "\n",
    "print(f\"Diabetes dataset: {df_train.shape}\")\n",
    "print(f\"Breast cancer dataset: {df_train_breast.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Subdivision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_diabetes_random_test.csv of shape (10604, 22)\n",
      "Saved: df_diabetes_random_1.csv of shape (20030, 22)\n",
      "Saved: df_diabetes_random_2.csv of shape (20029, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dariofenoglio/miniforge3/envs/CF_FL/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_diabetes_random_3.csv of shape (20029, 22)\n",
      "Saved: df_breast_random_test.csv of shape (86, 32)\n",
      "Saved: df_breast_random_1.csv of shape (161, 32)\n",
      "Saved: df_breast_random_2.csv of shape (161, 32)\n",
      "Saved: df_breast_random_3.csv of shape (161, 32)\n"
     ]
    }
   ],
   "source": [
    "# N institutions (5% out for testing)\n",
    "N = 3\n",
    "\n",
    "def random_split(df, N, file_prefix='df_diabetes'):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into N parts and saves each part as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to split.\n",
    "    N (int): Number of parts to split the DataFrame into.\n",
    "    file_prefix (str): Prefix for the output file names.\n",
    "    \"\"\"\n",
    "    # Shuffle the DataFrame\n",
    "    df_shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Leave out 5% for testing\n",
    "    df_train, df_test = train_test_split(df_shuffled, test_size=0.15, random_state=1)\n",
    "    df_test.to_csv(file_prefix + '_random_test.csv', index=False)\n",
    "    print(f'Saved: {file_prefix}_random_test.csv of shape {df_test.shape}')\n",
    "\n",
    "    # Split the DataFrame into N parts\n",
    "    df_splits = np.array_split(df_train, N)\n",
    "\n",
    "    # Save each part as a CSV file\n",
    "    for i, split in enumerate(df_splits, start=1):\n",
    "        filename = f'{file_prefix}_random_{i}.csv'\n",
    "        split.to_csv(filename, index=False)\n",
    "        print(f'Saved: {filename} of shape {split.shape}')\n",
    "\n",
    "\n",
    "random_split(df_train, N, file_prefix='df_diabetes')\n",
    "random_split(df_train_breast, N, file_prefix='df_breast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster based Subdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_diabetes_2cluster_test.csv of shape (10604, 22)\n",
      "N: 3\n",
      "Saved: df_diabetes_2cluster_1.csv of shape (8179, 22) pairs: 0 and 1\n",
      "Saved: df_diabetes_2cluster_2.csv of shape (46463, 22) pairs: 1 and 0\n",
      "Saved: df_diabetes_2cluster_3.csv of shape (5446, 22) pairs: 2 and 2\n",
      "Saved: df_breast_2cluster_test.csv of shape (86, 32)\n",
      "N: 3\n",
      "Saved: df_breast_2cluster_1.csv of shape (90, 32) pairs: 0 and 1\n",
      "Saved: df_breast_2cluster_2.csv of shape (229, 32) pairs: 2 and 0\n",
      "Saved: df_breast_2cluster_3.csv of shape (164, 32) pairs: 1 and 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "# Function to calculate Euclidean distances between centroids\n",
    "def centroid_distances(centroids0, centroids1):\n",
    "    N = len(centroids0)\n",
    "    print(f\"N: {N}\")\n",
    "    distances = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            distances[i, j] = np.linalg.norm(centroids0[i] - centroids1[j])\n",
    "    return distances\n",
    "\n",
    "# Function to calculate centroids\n",
    "def calculate_centroids(df, labels):\n",
    "    N = len(np.unique(labels))\n",
    "    centroids = []\n",
    "    for i in range(N):\n",
    "        centroids.append(df[labels == i].mean().to_numpy())\n",
    "    return centroids\n",
    "\n",
    "def cluster_by_class_split(df_train, N, file_prefix='df_diabetes'):\n",
    "    \"\"\"\n",
    "    In this code, distances will be a matrix where the element at [i, j] represents\n",
    "    the distance between the i-th cluster of class 0 and the j-th cluster of class 1.\n",
    "    The final matrix will be a N x N matrix, not simmetrical in general.\n",
    "    The following result means that for the first cluster of class 0, the second cluster \n",
    "    of class 1 is the closest one. For the second cluster of class 0, the third cluster of\n",
    "    class 1 is the closest one. And so on.\n",
    "    array([[22.52661847, 16.58598092, 30.50548191],\n",
    "       [ 4.33080647, 32.17891945, 25.41195157],\n",
    "       [27.11059815, 19.7759446 ,  8.12520036]])\n",
    "    \"\"\"\n",
    "\n",
    "    # Leave out 5% for testing\n",
    "    df_train, df_test = train_test_split(df_train, test_size=0.15, random_state=1)\n",
    "    df_test.to_csv(file_prefix + '_2cluster_test.csv', index=False)\n",
    "    print(f'Saved: {file_prefix}_2cluster_test.csv of shape {df_test.shape}')\n",
    "\n",
    "    # Splitting the dataset by class\n",
    "    df_train_0 = df_train[df_train['Labels'] == 0].drop('Labels', axis=1)\n",
    "    df_train_1 = df_train[df_train['Labels'] == 1].drop('Labels', axis=1)\n",
    "    # KMeans clustering\n",
    "    kmeans_0 = KMeans(n_clusters=N, random_state=1).fit(df_train_0)\n",
    "    kmeans_1 = KMeans(n_clusters=N, random_state=1).fit(df_train_1)\n",
    "    # Calculating centroids\n",
    "    centroids_0 = calculate_centroids(df_train_0, kmeans_0.labels_)\n",
    "    centroids_1 = calculate_centroids(df_train_1, kmeans_1.labels_)\n",
    "    # Calculating distances\n",
    "    distance_matrix = centroid_distances(centroids_0, centroids_1)  \n",
    "\n",
    "    # Pairing clusters\n",
    "    pairs = pair_clusters(distance_matrix)\n",
    "\n",
    "    # create the N clusters\n",
    "    i = 1\n",
    "    for c0,c1 in pairs:\n",
    "        df_0 = df_train[df_train['Labels'] == 0][kmeans_0.labels_ == c0]\n",
    "        df_1 = df_train[df_train['Labels'] == 1][kmeans_1.labels_ == c1]\n",
    "        # merge the clusters\n",
    "        df = pd.concat([df_0, df_1])\n",
    "        # randomize the order of the rows\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        # save the new dataset\n",
    "        filename = f'{file_prefix}_2cluster_{i}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f'Saved: {filename} of shape {df.shape} pairs: {c0} and {c1}')\n",
    "        i += 1\n",
    "\n",
    "def pair_clusters(dist_matrix):\n",
    "    distances_copy = copy.deepcopy(dist_matrix)\n",
    "    pairs = []\n",
    "    # cycle\n",
    "    while distances_copy.size > 0:\n",
    "        # Find the minimum value and its column index\n",
    "        min_value = np.min(distances_copy)\n",
    "        min_col_index = np.argmin(np.min(distances_copy, axis=0))\n",
    "        min_row_index = np.argmin(distances_copy[:, min_col_index])\n",
    "\n",
    "        # identify the real position \n",
    "        ind = np.where(dist_matrix == min_value) #print(\"Minimum value:\", min_value)#print(\"Column index of minimum value:\", ind[1])#print(\"Row index of minimum value:\", ind[0])\n",
    "\n",
    "        # record pairing \n",
    "        pairs.append((ind[1].item(0), ind[0].item(0)))  # (cluster_{min_col_index}_0, cluster_{min_row_index}_1)\n",
    "\n",
    "        # remove the paired clusters from further consideration\n",
    "        distances_copy = np.delete(distances_copy, min_row_index, axis=0)  # remove row\n",
    "        distances_copy = np.delete(distances_copy, min_col_index, axis=1)  # remove column\n",
    "\n",
    "    return pairs\n",
    "\n",
    "cluster_by_class_split(df_train, N, file_prefix='df_diabetes')\n",
    "cluster_by_class_split(df_train_breast, N, file_prefix='df_breast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_diabetes_cluster_test.csv of shape (10604, 22)\n",
      "Saved: df_diabetes_cluster_1.csv of shape (12405, 22)\n",
      "Saved: df_diabetes_cluster_2.csv of shape (9835, 22)\n",
      "Saved: df_diabetes_cluster_3.csv of shape (37848, 22)\n",
      "Saved: df_breast_cluster_test.csv of shape (86, 32)\n",
      "Saved: df_breast_cluster_1.csv of shape (364, 32)\n",
      "Saved: df_breast_cluster_2.csv of shape (18, 32)\n",
      "Saved: df_breast_cluster_3.csv of shape (101, 32)\n"
     ]
    }
   ],
   "source": [
    "# N institutions - clusters _ OLD VERSION\n",
    "N = 3\n",
    "\n",
    "def cluster_split(df, N, file_prefix='df_diabetes'):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into N clusters and saves each cluster as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to cluster.\n",
    "    N (int): Number of clusters to form.\n",
    "    file_prefix (str): Prefix for the output file names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Leave out 5% for testing\n",
    "    df_train, df_test = train_test_split(df, test_size=0.15, random_state=1)\n",
    "    df_test.to_csv(file_prefix + '_cluster_test.csv', index=False)\n",
    "    print(f'Saved: {file_prefix}_cluster_test.csv of shape {df_test.shape}')\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=N, random_state=1)\n",
    "    clusters = kmeans.fit_predict(df_train)\n",
    "\n",
    "    # Split the DataFrame based on clusters\n",
    "    for i in range(N):\n",
    "        cluster_df = df_train[clusters == i]\n",
    "        filename = f'{file_prefix}_cluster_{i+1}.csv'\n",
    "        cluster_df.to_csv(filename, index=False)\n",
    "        print(f'Saved: {filename} of shape {cluster_df.shape}')\n",
    "\n",
    "\n",
    "cluster_split(df_train, N, file_prefix='df_diabetes')\n",
    "cluster_split(df_train_breast, N, file_prefix='df_breast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double-Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes dataset\n",
      "Total shape 2cluster: 60088,22\n",
      "Total shape random: 60088,22\n",
      "Total shape cluster: 60088,22\n",
      "Test shape random: (10604, 22)\n",
      "Test shape 2cluster: (10604, 22)\n",
      "Test shape cluster: (10604, 22)\n",
      "\n",
      "Breast cancer dataset\n",
      "Total shape random: 483,32\n",
      "Total shape 2cluster: 483,32\n",
      "Total shape cluster: 483,32\n",
      "Test shape random: (86, 32)\n",
      "Test shape 2cluster: (86, 32)\n",
      "Test shape cluster: (86, 32)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "print(\"Diabetes dataset\")\n",
    "df1 = pd.read_csv('df_diabetes_2cluster_1.csv')\n",
    "df2 = pd.read_csv('df_diabetes_2cluster_2.csv')\n",
    "df3 = pd.read_csv('df_diabetes_2cluster_3.csv')\n",
    "print(f\"Total shape 2cluster: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "df1 = pd.read_csv('df_diabetes_random_1.csv')\n",
    "df2 = pd.read_csv('df_diabetes_random_2.csv')\n",
    "df3 = pd.read_csv('df_diabetes_random_3.csv')\n",
    "print(f\"Total shape random: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "df1 = pd.read_csv('df_diabetes_cluster_1.csv')\n",
    "df2 = pd.read_csv('df_diabetes_cluster_2.csv')\n",
    "df3 = pd.read_csv('df_diabetes_cluster_3.csv')\n",
    "print(f\"Total shape cluster: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "# print the shape of the data\n",
    "df1 = pd.read_csv('df_diabetes_random_test.csv')\n",
    "df2 = pd.read_csv('df_diabetes_2cluster_test.csv')\n",
    "df3 = pd.read_csv('df_diabetes_cluster_test.csv')\n",
    "print(f\"Test shape random: {df1.shape}\")\n",
    "print(f\"Test shape 2cluster: {df2.shape}\")\n",
    "print(f\"Test shape cluster: {df3.shape}\")\n",
    "\n",
    "# breast dataset\n",
    "print(\"\\nBreast cancer dataset\")\n",
    "df1 = pd.read_csv('df_breast_random_1.csv')\n",
    "df2 = pd.read_csv('df_breast_random_2.csv')\n",
    "df3 = pd.read_csv('df_breast_random_3.csv')\n",
    "print(f\"Total shape random: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "df1 = pd.read_csv('df_breast_2cluster_1.csv')\n",
    "df2 = pd.read_csv('df_breast_2cluster_2.csv')\n",
    "df3 = pd.read_csv('df_breast_2cluster_3.csv')\n",
    "print(f\"Total shape 2cluster: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "df1 = pd.read_csv('df_breast_cluster_1.csv')\n",
    "df2 = pd.read_csv('df_breast_cluster_2.csv')\n",
    "df3 = pd.read_csv('df_breast_cluster_3.csv')\n",
    "print(f\"Total shape cluster: {df1.shape[0] + df2.shape[0] + df3.shape[0]},{df1.shape[1]}\")\n",
    "\n",
    "# print the shape of the data\n",
    "df1 = pd.read_csv('df_breast_random_test.csv')\n",
    "df2 = pd.read_csv('df_breast_2cluster_test.csv')\n",
    "df3 = pd.read_csv('df_breast_cluster_test.csv')\n",
    "print(f\"Test shape random: {df1.shape}\")\n",
    "print(f\"Test shape 2cluster: {df2.shape}\")\n",
    "print(f\"Test shape cluster: {df3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>...</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556</td>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.01116</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273</td>\n",
       "      <td>9.742</td>\n",
       "      <td>15.67</td>\n",
       "      <td>61.50</td>\n",
       "      <td>289.9</td>\n",
       "      <td>0.09037</td>\n",
       "      <td>0.04689</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.01407</td>\n",
       "      <td>0.2081</td>\n",
       "      <td>...</td>\n",
       "      <td>20.88</td>\n",
       "      <td>68.09</td>\n",
       "      <td>355.2</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.04043</td>\n",
       "      <td>0.05159</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.08175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>19.550</td>\n",
       "      <td>28.77</td>\n",
       "      <td>133.60</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>0.09260</td>\n",
       "      <td>0.20630</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>...</td>\n",
       "      <td>36.27</td>\n",
       "      <td>178.60</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>0.1281</td>\n",
       "      <td>0.5329</td>\n",
       "      <td>0.42510</td>\n",
       "      <td>0.19410</td>\n",
       "      <td>0.2818</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168</td>\n",
       "      <td>17.470</td>\n",
       "      <td>24.68</td>\n",
       "      <td>116.10</td>\n",
       "      <td>984.6</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.16030</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1538</td>\n",
       "      <td>...</td>\n",
       "      <td>32.33</td>\n",
       "      <td>155.30</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>0.1376</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.48900</td>\n",
       "      <td>0.17210</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.09300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340</td>\n",
       "      <td>14.420</td>\n",
       "      <td>16.54</td>\n",
       "      <td>94.15</td>\n",
       "      <td>641.2</td>\n",
       "      <td>0.09751</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.080070</td>\n",
       "      <td>0.04223</td>\n",
       "      <td>0.1912</td>\n",
       "      <td>...</td>\n",
       "      <td>21.51</td>\n",
       "      <td>111.40</td>\n",
       "      <td>862.1</td>\n",
       "      <td>0.1294</td>\n",
       "      <td>0.3371</td>\n",
       "      <td>0.37550</td>\n",
       "      <td>0.14140</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>0.08764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
       "0         556   10.160     19.59       64.73   311.7      0.10030   \n",
       "1         273    9.742     15.67       61.50   289.9      0.09037   \n",
       "2         256   19.550     28.77      133.60  1207.0      0.09260   \n",
       "3         168   17.470     24.68      116.10   984.6      0.10490   \n",
       "4         340   14.420     16.54       94.15   641.2      0.09751   \n",
       "\n",
       "   compactness1  concavity1  concave_points1  symmetry1  ...  texture3  \\\n",
       "0       0.07504    0.005025          0.01116     0.1791  ...     22.88   \n",
       "1       0.04689    0.011030          0.01407     0.2081  ...     20.88   \n",
       "2       0.20630    0.178400          0.11440     0.1893  ...     36.27   \n",
       "3       0.16030    0.215900          0.10430     0.1538  ...     32.33   \n",
       "4       0.11390    0.080070          0.04223     0.1912  ...     21.51   \n",
       "\n",
       "   perimeter3   area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
       "0       67.88   347.3       0.1265        0.1200     0.01005          0.02232   \n",
       "1       68.09   355.2       0.1467        0.0937     0.04043          0.05159   \n",
       "2      178.60  1926.0       0.1281        0.5329     0.42510          0.19410   \n",
       "3      155.30  1660.0       0.1376        0.3830     0.48900          0.17210   \n",
       "4      111.40   862.1       0.1294        0.3371     0.37550          0.14140   \n",
       "\n",
       "   symmetry3  fractal_dimension3  Labels  \n",
       "0     0.2262             0.06742       0  \n",
       "1     0.2841             0.08175       0  \n",
       "2     0.2818             0.10050       1  \n",
       "3     0.2160             0.09300       1  \n",
       "4     0.3053             0.08764       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>...</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>421</td>\n",
       "      <td>14.69</td>\n",
       "      <td>13.98</td>\n",
       "      <td>98.22</td>\n",
       "      <td>656.1</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.18360</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.06300</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>...</td>\n",
       "      <td>18.34</td>\n",
       "      <td>114.10</td>\n",
       "      <td>809.2</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.36350</td>\n",
       "      <td>0.3219</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.09208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>...</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.41660</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>292</td>\n",
       "      <td>12.95</td>\n",
       "      <td>16.02</td>\n",
       "      <td>83.14</td>\n",
       "      <td>513.7</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0.07943</td>\n",
       "      <td>0.06155</td>\n",
       "      <td>0.03370</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>...</td>\n",
       "      <td>19.93</td>\n",
       "      <td>88.81</td>\n",
       "      <td>585.4</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.20680</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.10560</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.09584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186</td>\n",
       "      <td>18.31</td>\n",
       "      <td>18.58</td>\n",
       "      <td>118.60</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.08588</td>\n",
       "      <td>0.08468</td>\n",
       "      <td>0.08169</td>\n",
       "      <td>0.05814</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>...</td>\n",
       "      <td>26.36</td>\n",
       "      <td>139.20</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.24450</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.06938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>414</td>\n",
       "      <td>15.13</td>\n",
       "      <td>29.81</td>\n",
       "      <td>96.71</td>\n",
       "      <td>719.5</td>\n",
       "      <td>0.08320</td>\n",
       "      <td>0.04605</td>\n",
       "      <td>0.04686</td>\n",
       "      <td>0.02739</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>...</td>\n",
       "      <td>36.91</td>\n",
       "      <td>110.10</td>\n",
       "      <td>931.4</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.06575</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.06165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
       "0         421    14.69     13.98       98.22   656.1      0.10310   \n",
       "1          47    13.17     18.66       85.98   534.6      0.11580   \n",
       "2         292    12.95     16.02       83.14   513.7      0.10050   \n",
       "3         186    18.31     18.58      118.60  1041.0      0.08588   \n",
       "4         414    15.13     29.81       96.71   719.5      0.08320   \n",
       "\n",
       "   compactness1  concavity1  concave_points1  symmetry1  ...  texture3  \\\n",
       "0       0.18360     0.14500          0.06300     0.2086  ...     18.34   \n",
       "1       0.12310     0.12260          0.07340     0.2128  ...     27.95   \n",
       "2       0.07943     0.06155          0.03370     0.1730  ...     19.93   \n",
       "3       0.08468     0.08169          0.05814     0.1621  ...     26.36   \n",
       "4       0.04605     0.04686          0.02739     0.1852  ...     36.91   \n",
       "\n",
       "   perimeter3   area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
       "0      114.10   809.2       0.1312       0.36350      0.3219          0.11080   \n",
       "1      102.80   759.4       0.1786       0.41660      0.5006          0.20880   \n",
       "2       88.81   585.4       0.1483       0.20680      0.2241          0.10560   \n",
       "3      139.20  1410.0       0.1234       0.24450      0.3538          0.15710   \n",
       "4      110.10   931.4       0.1148       0.09866      0.1547          0.06575   \n",
       "\n",
       "   symmetry3  fractal_dimension3  Labels  \n",
       "0     0.2827             0.09208       0  \n",
       "1     0.3900             0.11790       1  \n",
       "2     0.3380             0.09584       0  \n",
       "3     0.3206             0.06938       1  \n",
       "4     0.3233             0.06165       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
       "1    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
       "2    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
       "3    11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
       "4    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
       "0      0.3001          0.14710     0.2419             0.07871  ...    25.38   \n",
       "1      0.0869          0.07017     0.1812             0.05667  ...    24.99   \n",
       "2      0.1974          0.12790     0.2069             0.05999  ...    23.57   \n",
       "3      0.2414          0.10520     0.2597             0.09744  ...    14.91   \n",
       "4      0.1980          0.10430     0.1809             0.05883  ...    22.54   \n",
       "\n",
       "   texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0     17.33      184.60  2019.0       0.1622        0.6656      0.7119   \n",
       "1     23.41      158.80  1956.0       0.1238        0.1866      0.2416   \n",
       "2     25.53      152.50  1709.0       0.1444        0.4245      0.4504   \n",
       "3     26.50       98.87   567.7       0.2098        0.8663      0.6869   \n",
       "4     16.67      152.20  1575.0       0.1374        0.2050      0.4000   \n",
       "\n",
       "   concave_points3  symmetry3  fractal_dimension3  \n",
       "0           0.2654     0.4601             0.11890  \n",
       "1           0.1860     0.2750             0.08902  \n",
       "2           0.2430     0.3613             0.08758  \n",
       "3           0.2575     0.6638             0.17300  \n",
       "4           0.1625     0.2364             0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"utils.py\"), '..'))\n",
    "import utils\n",
    "\n",
    "# Set random seed and Use 'cuda' GPU\n",
    "device = utils.check_gpu(manual_seed=True)\n",
    "\n",
    "X = pd.read_csv('X_breast.csv', index_col=0)\n",
    "Y = pd.read_csv('y_breast.csv', index_col=0)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map 'M' to 1 and 'B' to 0\n",
    "Y['Diagnosis'] = Y['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10 % of total data as Test set and the rest as (Train + Validation) set \n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "# Use 20 % of (Train + Validation) set as Validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "X_val = scaler.transform(X_val.values)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "X_val = torch.tensor(X_val).float().to(device)\n",
    "\n",
    "y_train = torch.LongTensor(y_train.values).to(device).squeeze()\n",
    "y_val = torch.LongTensor(y_val.values).to(device).squeeze()\n",
    "\n",
    "# Hyperparameter\n",
    "learning_rate = 1e-1   ### After tuning?\n",
    "n_epochs = 500\n",
    "drop_prob = 0.3\n",
    "\n",
    "def randomize_class(a, include=True):\n",
    "        # Get the number of classes and the number of samples\n",
    "        num_classes = a.size(1)\n",
    "        num_samples = a.size(0)\n",
    "\n",
    "        # Generate random indices for each row to place 1s, excluding the original positions\n",
    "        random_indices = torch.randint(0, num_classes, (num_samples,)).to(a.device)\n",
    "\n",
    "        # Ensure that the generated indices are different from the original positions\n",
    "        # TODO we inclue also same label to make sure that every class is represented \n",
    "        if not include:\n",
    "            original_indices = a.argmax(dim=1)\n",
    "            random_indices = torch.where(random_indices == original_indices, (random_indices + 1) % num_classes, random_indices)\n",
    "\n",
    "        # Create a second tensor with 1s at the random indices\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(num_samples), random_indices] = 1\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "EPS = 1e-9\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        input_dim = 30\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.concept_mean_predictor = torch.nn.Sequential(torch.nn.Linear(input_dim, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))  # 20 is fine?\n",
    "        self.concept_var_predictor = torch.nn.Sequential(torch.nn.Linear(input_dim, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))\n",
    "        self.decoder = torch.nn.Sequential(torch.nn.Linear(20, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, input_dim))\n",
    "        self.concept_mean_z3_predictor = torch.nn.Sequential(torch.nn.Linear(20 + input_dim + 2, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))\n",
    "        self.concept_var_z3_predictor = torch.nn.Sequential(torch.nn.Linear(20 + input_dim + 2, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))\n",
    "        self.concept_mean_qz3_predictor = torch.nn.Sequential(torch.nn.Linear(20 + input_dim + 4 + input_dim, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))\n",
    "        self.concept_var_qz3_predictor = torch.nn.Sequential(torch.nn.Linear(20 + input_dim + 4 + input_dim, 128), torch.nn.LeakyReLU(), torch.nn.Linear(128, 20))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "#         self.mask = torch.nn.Parameter(torch.Tensor([0,0,0,0,0,1,0,0,0,0,\n",
    "#                                   0,0,0,0,0,0,0,1,1,1,1]), requires_grad=False)\n",
    "        mean = 0.5\n",
    "        self.mask = torch.nn.Parameter(torch.Tensor([mean,mean,mean,mean,mean,mean,mean,mean,mean,1,mean,mean,mean,mean,\n",
    "                                  mean,mean,mean,mean,mean,mean,mean,1,1,1,1,1,1,1,1,1]), requires_grad=False)\n",
    "        std = 1\n",
    "        self.mask_std = torch.nn.Parameter(torch.Tensor([std,std,std,std,std,std,std,std,std,0.00001,std,std,std,std,\n",
    "                                      std,std,std,std,std,std,std,0.00001,0.00001,0.00001,0.00001,0.00001,0.00001,0.00001,0.00001,0.00001]), requires_grad=False)\n",
    "        self.binary_feature = torch.nn.Parameter(torch.Tensor(\n",
    "                            [1,1,1,0,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0]).bool(), requires_grad=False)\n",
    "        self.mask.to(device)\n",
    "        self.mask_std.to(device)\n",
    "        \n",
    "        self.distr_mask = torch.distributions.Normal(self.mask, self.mask_std)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                \n",
    "    def get_mask(self, x):\n",
    "        mask = torch.rand(x.shape).to(device)\n",
    "        return mask\n",
    "                \n",
    "    def multiple_cf(self, x, include=False, n=1):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc5(out)\n",
    "        \n",
    "        z2_mu = self.concept_mean_predictor(x)\n",
    "        z2_log_var = self.concept_var_predictor(x)\n",
    "        z2_sigma = torch.exp(z2_log_var / 2) + EPS\n",
    "        qz2_x = torch.distributions.Normal(z2_mu, z2_sigma)\n",
    "        z2 = qz2_x.rsample()\n",
    "        p_z2 = torch.distributions.Normal(torch.zeros_like(qz2_x.mean), torch.ones_like(qz2_x.mean))\n",
    "\n",
    "        x_reconstructed = self.decoder(z2)\n",
    "#         x_reconstructed = torch.clamp(x_reconstructed, min=0, max=1)\n",
    "#         x_reconstructed[:, self.binary_feature] = torch.sigmoid(x_reconstructed[:, self.binary_feature])\n",
    "#         x_reconstructed[:, ~self.binary_feature] = torch.clamp(x_reconstructed[:, ~self.binary_feature], min=0, max=1)\n",
    "        \n",
    "        y_prime = randomize_class((out).float(), include=include)\n",
    "        \n",
    "        z2_c_y_y_prime = torch.cat((z2, x, out, y_prime, ), dim=1)\n",
    "        z3_mu = self.concept_mean_qz3_predictor(z2_c_y_y_prime)\n",
    "        z3_log_var = self.concept_var_qz3_predictor(z2_c_y_y_prime)\n",
    "        z3_sigma = torch.exp(z3_log_var / 2) + EPS\n",
    "        qz3_z2_c_y_y_prime = torch.distributions.Normal(z3_mu, z3_sigma)\n",
    "        z3 = qz3_z2_c_y_y_prime.rsample((n,))\n",
    "        \n",
    "        z2_c_y = torch.cat((z2, x, out), dim=1)\n",
    "        z3_mu = self.concept_mean_z3_predictor(z2_c_y)\n",
    "        z3_log_var = self.concept_var_z3_predictor(z2_c_y)\n",
    "        z3_sigma = torch.exp(z3_log_var / 2) + EPS\n",
    "        pz3_z2_c_y = torch.distributions.Normal(z3_mu, z3_sigma)\n",
    "        \n",
    "        x_prime_reconstructed = self.decoder(z3)\n",
    "#         x_prime_reconstructed = torch.clamp(x_prime_reconstructed, min=0, max=1)\n",
    "#         x_prime_reconstructed[:, :, self.binary_feature] = torch.sigmoid(x_prime_reconstructed[:, :, self.binary_feature])\n",
    "#         x_prime_reconstructed[:, :, ~self.binary_feature] = torch.clamp(x_prime_reconstructed[:, :, ~self.binary_feature], min=0, max=1)\n",
    "        \n",
    "        x_prime_reconstructed = x_prime_reconstructed * (1 - self.mask) + (x * self.mask)\n",
    "        out2 = self.fc1(x_prime_reconstructed)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc2(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc3(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc4(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc5(out2)\n",
    "        \n",
    "        return out, x_reconstructed, qz2_x, p_z2, out2, x_prime_reconstructed, qz3_z2_c_y_y_prime, pz3_z2_c_y, y_prime\n",
    "                \n",
    "    def forward(self, x, include=True, n=1, mask_init=None):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc5(out)\n",
    "        \n",
    "        z2_mu = self.concept_mean_predictor(x)\n",
    "        z2_log_var = self.concept_var_predictor(x)\n",
    "        z2_sigma = torch.exp(z2_log_var / 2) + EPS\n",
    "        qz2_x = torch.distributions.Normal(z2_mu, z2_sigma)\n",
    "        z2 = qz2_x.rsample()\n",
    "        p_z2 = torch.distributions.Normal(torch.zeros_like(qz2_x.mean), torch.ones_like(qz2_x.mean))\n",
    "\n",
    "        x_reconstructed = self.decoder(z2)\n",
    "        x_reconstructed = F.hardtanh(x_reconstructed, -0.1, 1.1)\n",
    "#         x_reconstructed = torch.clamp(x_reconstructed, min=0, max=1)\n",
    "#         x_reconstructed[:, self.binary_feature] = torch.sigmoid(x_reconstructed[:, self.binary_feature])\n",
    "#         x_reconstructed[:, ~self.binary_feature] = torch.clamp(x_reconstructed[:, ~self.binary_feature], min=0, max=1)\n",
    "        \n",
    "        y_prime = randomize_class((out).float(), include=include)\n",
    "        \n",
    "        if self.training:\n",
    "#             mask = self.distr_mask.sample((y_prime.shape[0],))\n",
    "#             mask.to('cuda')\n",
    "#             mask = torch.clip(mask, min=0, max=1)\n",
    "            mask = self.get_mask(x)\n",
    "#             mask = self.mask\n",
    "        else:\n",
    "            if mask_init is not None:\n",
    "                mask = mask_init\n",
    "                mask = mask.to(device)\n",
    "                mask = mask.repeat(y_prime.shape[0], 1)\n",
    "            else:\n",
    "                mask = self.get_mask(x)\n",
    "#                 mask = self.distr_mask.sample((y_prime.shape[0],))\n",
    "#                 mask = torch.clip(mask, min=0, max=1)\n",
    "#                 mask = self.mask\n",
    "#         mask = mask.repeat(y_prime.shape[0], 1)\n",
    "        \n",
    "        z2_c_y_y_prime = torch.cat((z2, x, out, y_prime, mask), dim=1)\n",
    "        z3_mu = self.concept_mean_qz3_predictor(z2_c_y_y_prime)\n",
    "        z3_log_var = self.concept_var_qz3_predictor(z2_c_y_y_prime)\n",
    "        z3_sigma = torch.exp(z3_log_var / 2) + EPS\n",
    "        qz3_z2_c_y_y_prime = torch.distributions.Normal(z3_mu, z3_sigma)\n",
    "        z3 = qz3_z2_c_y_y_prime.rsample((n,))\n",
    "        \n",
    "        if n == 1: \n",
    "            z3 = z3.squeeze(0)\n",
    "            \n",
    "            \n",
    "        z2_c_y = torch.cat((z2, x, out), dim=1)\n",
    "        z3_mu = self.concept_mean_z3_predictor(z2_c_y)\n",
    "        z3_log_var = self.concept_var_z3_predictor(z2_c_y)\n",
    "        z3_sigma = torch.exp(z3_log_var / 2) + EPS\n",
    "        pz3_z2_c_y = torch.distributions.Normal(z3_mu, z3_sigma)\n",
    "        \n",
    "        x_prime_reconstructed = self.decoder(z3)\n",
    "        x_prime_reconstructed = F.hardtanh(x_prime_reconstructed, -0.1, 1.1)\n",
    "#         x_prime_reconstructed = torch.clamp(x_prime_reconstructed, min=0, max=1)\n",
    "#         x_prime_reconstructed[:, self.binary_feature] = torch.sigmoid(x_prime_reconstructed[:, self.binary_feature])\n",
    "#         x_prime_reconstructed[:, ~self.binary_feature] = torch.clamp(x_prime_reconstructed[:, ~self.binary_feature], min=0, max=1)\n",
    "        \n",
    "        x_prime_reconstructed = x_prime_reconstructed * (1 - mask) + (x * mask)\n",
    "        \n",
    "        if not self.training:\n",
    "#             print(x_prime_reconstructed[:2, 3])\n",
    "            x_prime_reconstructed = torch.clamp(x_prime_reconstructed, min=0, max=1)\n",
    "            x_prime_reconstructed = scaler.inverse_transform(x_prime_reconstructed.detach().cpu().numpy())\n",
    "            x_prime_reconstructed = np.round(x_prime_reconstructed)\n",
    "            x_prime_reconstructed = scaler.transform(x_prime_reconstructed)\n",
    "            x_prime_reconstructed = torch.Tensor(x_prime_reconstructed).to(device)\n",
    "            \n",
    "        out2 = self.fc1(x_prime_reconstructed)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc2(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc3(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc4(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        \n",
    "        out2 = self.fc5(out2)\n",
    "    \n",
    "        return out, x_reconstructed, qz2_x, p_z2, out2, x_prime_reconstructed, qz3_z2_c_y_y_prime, pz3_z2_c_y, y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at epoch: 1 with Validity: 0.4757281541824341\n",
      "Epoch   50 / 1000, Cost : 1.4673, Acc : 96.33 %, Validity : 52.32 %, Val Cost : 0.2249, Val Acc : 93.20 % , Val Validity : 31.07 %\n",
      "Epoch  100 / 1000, Cost : 1.1949, Acc : 97.80 %, Validity : 53.55 %, Val Cost : 0.1733, Val Acc : 97.09 % , Val Validity : 33.98 %\n",
      "Epoch  150 / 1000, Cost : 0.9952, Acc : 98.53 %, Validity : 63.33 %, Val Cost : 0.1391, Val Acc : 97.09 % , Val Validity : 34.95 %\n",
      "Epoch  200 / 1000, Cost : 0.8394, Acc : 98.29 %, Validity : 80.44 %, Val Cost : 0.1074, Val Acc : 97.09 % , Val Validity : 36.89 %\n",
      "Epoch  250 / 1000, Cost : 0.7544, Acc : 98.53 %, Validity : 90.71 %, Val Cost : 0.1139, Val Acc : 96.12 % , Val Validity : 37.86 %\n",
      "Epoch  300 / 1000, Cost : 0.7103, Acc : 98.53 %, Validity : 90.71 %, Val Cost : 0.1044, Val Acc : 96.12 % , Val Validity : 39.81 %\n",
      "Saved model at epoch: 329 with Validity: 0.5048543810844421\n",
      "Epoch  350 / 1000, Cost : 0.6811, Acc : 98.78 %, Validity : 91.93 %, Val Cost : 0.1013, Val Acc : 96.12 % , Val Validity : 41.75 %\n",
      "Epoch  400 / 1000, Cost : 0.6572, Acc : 99.02 %, Validity : 90.71 %, Val Cost : 0.0917, Val Acc : 97.09 % , Val Validity : 42.72 %\n",
      "Epoch  450 / 1000, Cost : 0.6249, Acc : 99.02 %, Validity : 92.91 %, Val Cost : 0.0946, Val Acc : 97.09 % , Val Validity : 49.51 %\n",
      "Epoch  500 / 1000, Cost : 0.6266, Acc : 99.02 %, Validity : 93.15 %, Val Cost : 0.1104, Val Acc : 96.12 % , Val Validity : 38.83 %\n",
      "Saved model at epoch: 534 with Validity: 0.5339806079864502\n",
      "Epoch  550 / 1000, Cost : 0.6158, Acc : 99.27 %, Validity : 92.67 %, Val Cost : 0.1027, Val Acc : 96.12 % , Val Validity : 42.72 %\n",
      "Epoch  600 / 1000, Cost : 0.6032, Acc : 99.27 %, Validity : 93.89 %, Val Cost : 0.0998, Val Acc : 96.12 % , Val Validity : 39.81 %\n",
      "Epoch  650 / 1000, Cost : 0.5736, Acc : 99.27 %, Validity : 92.91 %, Val Cost : 0.0920, Val Acc : 96.12 % , Val Validity : 41.75 %\n",
      "Epoch  700 / 1000, Cost : 0.5850, Acc : 99.51 %, Validity : 92.42 %, Val Cost : 0.0952, Val Acc : 96.12 % , Val Validity : 50.49 %\n",
      "Epoch  750 / 1000, Cost : 0.5588, Acc : 99.51 %, Validity : 94.62 %, Val Cost : 0.0983, Val Acc : 96.12 % , Val Validity : 40.78 %\n",
      "Epoch  800 / 1000, Cost : 0.5474, Acc : 99.76 %, Validity : 93.64 %, Val Cost : 0.0974, Val Acc : 96.12 % , Val Validity : 51.46 %\n",
      "Epoch  850 / 1000, Cost : 0.5911, Acc : 99.27 %, Validity : 89.73 %, Val Cost : 0.1002, Val Acc : 96.12 % , Val Validity : 42.72 %\n",
      "Epoch  900 / 1000, Cost : 0.5397, Acc : 99.27 %, Validity : 93.40 %, Val Cost : 0.0874, Val Acc : 96.12 % , Val Validity : 46.60 %\n",
      "Epoch  950 / 1000, Cost : 0.5446, Acc : 99.51 %, Validity : 93.15 %, Val Cost : 0.0893, Val Acc : 96.12 % , Val Validity : 42.72 %\n",
      "Epoch 1000 / 1000, Cost : 0.5735, Acc : 99.51 %, Validity : 89.98 %, Val Cost : 0.0936, Val Acc : 96.12 % , Val Validity : 49.51 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "train_loss = list()\n",
    "val_loss = list()\n",
    "\n",
    "learning_rate = 1e-2 \n",
    "n_epochs = 1000\n",
    "best_model_round = 0\n",
    "best_val = 0\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    H, x_reconstructed, q, p, H2, x_prime, q_prime, p_prime, y_prime = model(X_train)\n",
    "    loss_task = loss_fn(H, y_train)\n",
    "    loss_kl = torch.distributions.kl_divergence(p, q).mean()\n",
    "    loss_rec = F.mse_loss(x_reconstructed, X_train, reduction='mean')\n",
    "    loss_validity = loss_fn(H2, y_prime.argmax(dim=-1))\n",
    "    loss_kl2 = torch.distributions.kl_divergence(p_prime, q_prime).mean() \n",
    "    loss_p_d = torch.distributions.kl_divergence(p, p_prime).mean() \n",
    "    loss_q_d = torch.distributions.kl_divergence(q, q_prime).mean()\n",
    "    \n",
    "    \n",
    "    #loss_h_d = ((torch.abs(X_train - x_prime)) * mask).mean()\n",
    "    \n",
    "    lambda1 = 2 # loss parameter for kl divergence p-q and p_prime-q_prime\n",
    "    lambda2 = 10 # loss parameter for input reconstruction\n",
    "    lambda3 = 0.5 # loss parameter for validity of counterfactuals\n",
    "    lambda4 = 0 # loss parameter for creating counterfactuals that are closer to the initial input\n",
    "    #             increasing it, decrease the validity of counterfactuals. It is expected and makes sense.\n",
    "    #             It is a design choice to have better counterfactuals or closer counterfactuals.\n",
    "    loss = loss_task + lambda1*loss_kl + lambda2*loss_rec + lambda3*loss_validity + lambda1*loss_kl2 + loss_p_d + lambda4*loss_q_d #+ 0*loss_h_d\n",
    "    train_loss.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    acc = (torch.argmax(H, dim=1) == y_train).float().mean().item()\n",
    "    acc_prime = (torch.argmax(H2, dim=1) == y_prime.argmax(dim=-1)).float().mean().item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        H_val, x_reconstructed, q, p, H2, x_prime, q_prime, p_prime, y_prime = model(X_val, False)\n",
    "        loss_val = loss_fn(H_val, y_val)\n",
    "        acc_val = (torch.argmax(H_val, dim=1) == y_val).float().mean().item()\n",
    "        acc_prime_val = (torch.argmax(H2, dim=1) == y_prime.argmax(dim=-1)).float().mean().item()\n",
    "        \n",
    "        val_loss.append(loss_val.item())\n",
    "        if acc_prime_val > best_val:\n",
    "            print(f\"Saved model at epoch: {epoch} with Validity: {acc_prime_val}\")\n",
    "            best_val = acc_prime_val\n",
    "            best_model_round = epoch\n",
    "            torch.save(model.state_dict(), f\"checkpoints/model_round.pth\")\n",
    "        \n",
    "    if epoch % 50 == 0:\n",
    "        print('Epoch {:4d} / {}, Cost : {:.4f}, Acc : {:.2f} %, Validity : {:.2f} %, Val Cost : {:.4f}, Val Acc : {:.2f} % , Val Validity : {:.2f} %'.format(\n",
    "            epoch, n_epochs, loss.item(), acc*100, acc_prime*100, loss_val.item(), acc_val*100, acc_prime_val*100))\n",
    "        \n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/model_round.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CF_FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
